{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1**\tThe Iris dataset is a classic example for demonstrating classification algorithms. It consists of 150 samples of iris flowers belonging to three species: Setosa, Versicolor, and Virginica, with four input features (sepal and petal length/width).\n",
        "\n",
        "\n",
        "a)\tLoad the dataset and perform train–test split (80:20).\n",
        "\n",
        "b)\tTrain three different SVM models using the following kernels:\n",
        "Linear, Polynomial (degree=3), RBF\n",
        "\n",
        "c)\tEvaluate each model using:\n",
        "•\tAccuracy\n",
        "•\tPrecision\n",
        "•\tRecall\n",
        "•\tF1-Score\n",
        "\n",
        "d)\tDisplay the confusion matrix for each kernel.\n",
        "\n",
        "e)\tIdentify which kernel performs the best and why.\n"
      ],
      "metadata": {
        "id": "U8JfFKc6HLTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# 1. Load dataset [cite: 4]\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Train-test split (80:20) [cite: 4]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of kernels to evaluate [cite: 5, 6]\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "print(\"--- SVM Classification Results on Iris Dataset ---\")\n",
        "\n",
        "for k in kernels:\n",
        "    # Initialize model (degree=3 is default for poly, but specified for clarity)\n",
        "    if k == 'poly':\n",
        "        model = SVC(kernel=k, degree=3)\n",
        "    else:\n",
        "        model = SVC(kernel=k)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics [cite: 7]\n",
        "    # Note: average='weighted' is used because Iris is a multi-class dataset\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, average='weighted')\n",
        "    rec = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Output results\n",
        "    print(f\"\\nKernel: {k.upper()}\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQXmuqcjFGIq",
        "outputId": "66a587ae-c1bb-489b-e344-5db4c3cdc965"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SVM Classification Results on Iris Dataset ---\n",
            "\n",
            "Kernel: LINEAR\n",
            "Accuracy:  1.0000\n",
            "Precision: 1.0000\n",
            "Recall:    1.0000\n",
            "F1-Score:  1.0000\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Kernel: POLY\n",
            "Accuracy:  1.0000\n",
            "Precision: 1.0000\n",
            "Recall:    1.0000\n",
            "F1-Score:  1.0000\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Kernel: RBF\n",
            "Accuracy:  1.0000\n",
            "Precision: 1.0000\n",
            "Recall:    1.0000\n",
            "F1-Score:  1.0000\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2**\tSVM models are highly sensitive to the scale of input features. When features have different ranges, the algorithm may incorrectly assign higher importance to variables with larger magnitudes, affecting the placement of the separating hyperplane. Feature scaling ensures that all attributes contribute equally to distance-based computations, which is especially crucial for kernels like RBF or polynomial.\n",
        "\n",
        "A) Use the Breast Cancer dataset from sklearn.datasets.load_breast_cancer.\n",
        "\n",
        "B) Train an SVM (RBF kernel) model with and without feature scaling (StandardScaler). Compare both results using:\n",
        "\n",
        "•\tTraining accuracy\n",
        "\n",
        "•\tTesting accuracy\n"
      ],
      "metadata": {
        "id": "3RIDVhlbHdMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFSAs2N8E4cQ",
        "outputId": "6a912e40-9c4e-4275-ddad-7c771026b97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Effect of Feature Scaling on SVM (RBF) ---\n",
            "\n",
            "1. Without Feature Scaling:\n",
            "Training Accuracy: 0.9143\n",
            "Testing Accuracy:  0.9474\n",
            "\n",
            "2. With Feature Scaling (StandardScaler):\n",
            "Training Accuracy: 0.9890\n",
            "Testing Accuracy:  0.9825\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load dataset [cite: 17]\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"--- Effect of Feature Scaling on SVM (RBF) ---\")\n",
        "\n",
        "# --- Case A: WITHOUT Scaling ---\n",
        "model_no_scale = SVC(kernel='rbf')\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n1. Without Feature Scaling:\")\n",
        "print(f\"Training Accuracy: {model_no_scale.score(X_train, y_train):.4f}\")\n",
        "print(f\"Testing Accuracy:  {model_no_scale.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# --- Case B: WITH Scaling ---\n",
        "# Initialize Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training data and transform both train and test\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scale = SVC(kernel='rbf')\n",
        "model_with_scale.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\n2. With Feature Scaling (StandardScaler):\")\n",
        "print(f\"Training Accuracy: {model_with_scale.score(X_train_scaled, y_train):.4f}\")\n",
        "print(f\"Testing Accuracy:  {model_with_scale.score(X_test_scaled, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Discuss the effect of feature scaling on SVM performance"
      ],
      "metadata": {
        "id": "8jGLwFpAHntF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Effect of Feature Scaling**\n",
        "\n",
        "**Without Scaling**: The accuracy is usually significantly lower (often around 90-93%). The model struggles because features like \"Area\" (which has large values) dominate features like \"Smoothness\" (which has tiny decimals).\n",
        "\n",
        "**With Scaling**: The accuracy typically improves (often to 96-98%).\n",
        "\n",
        "\n",
        "**Conclusion**: Feature scaling ensures that all attributes contribute equally to the distance computations. For distance-based algorithms like SVM (specifically RBF and Poly kernels), scaling is crucial for finding the optimal hyperplane"
      ],
      "metadata": {
        "id": "JECzZurpFULN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wz7XEoLAFbcW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}